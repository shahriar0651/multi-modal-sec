{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This repo contains the following:\n",
    "- Code for CNN-based classification for MNIST Dataset\n",
    "- Code for Pointnet classification for MNIST3D Dataset\n",
    "- Code for Fusion-based classification for MNIST and MNIST3D Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up and load libraries\n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Tensorflow and Keras...\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "# plot and others...\n",
    "from matplotlib import pyplot as plt\n",
    "import trimesh\n",
    "import seaborn as sns\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from sklearn.metrics import classification_report\n",
    "# set seed\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cloud_points = 1024\n",
    "point_cloud_size = (num_cloud_points, 3)\n",
    "image_size = (30, 30, 1)\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "patience = 2\n",
    "\n",
    "# run_type = 'demo'\n",
    "run_type = 'full'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST (2D + 3D) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory\n",
    "DATADIR = '../data/multimodal/mnist3d/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_type, num_pcs):\n",
    "    '''\n",
    "    Load train and test data: 2D images, 3D point clouds and labels\n",
    "\n",
    "    arg:\n",
    "    data_type: 'train' or 'test'\n",
    "\n",
    "    return:\n",
    "    x_2d: 2D images \n",
    "    x_3d: 3D pointcloud\n",
    "    y: labels\n",
    "    '''\n",
    "    print(f\"Loading {data_type} data......\")\n",
    "\n",
    "    with h5py.File(DATADIR + f'{data_type}_point_clouds.h5', 'r') as points_dataset:\n",
    "\n",
    "        # Array to store data...\n",
    "        x_2d = []\n",
    "        x_3d = []\n",
    "        y = [] \n",
    "\n",
    "        # Looping over the whole dataset..\n",
    "        for i, (key, sample) in enumerate(points_dataset.items()):\n",
    "            \n",
    "            # Saving images...\n",
    "            x_2d.append(sample['img'][:])\n",
    "\n",
    "            # Sampling point cloud and saving...\n",
    "            pointCloud = sample['points'][:]\n",
    "            num_of_rows = pointCloud.shape[0]\n",
    "            random_ind = np.random.choice(num_of_rows, size=num_pcs, replace=False)\n",
    "            pointCloud = pointCloud[random_ind, :]\n",
    "            x_3d.append(pointCloud)\n",
    "\n",
    "            # Ssaving labels...\n",
    "            y.append(sample.attrs['label'])\n",
    "\n",
    "    # converting to np array...\n",
    "    x_2d = np.stack(x_2d).reshape(-1, 30, 30, 1)\n",
    "    x_3d = np.stack(x_3d)\n",
    "    \n",
    "    # convert class vectors to binary class matrices\n",
    "    y = keras.utils.to_categorical(np.array(y), num_classes)  \n",
    "\n",
    "    # Dropping samples if running as a demo...\n",
    "    \n",
    "    # Total samples..............\n",
    "    total_samples = x_3d.shape[0]\n",
    "    # Taking random indices for resampling\n",
    "    random_indices = np.arange(total_samples)\n",
    "    np.random.shuffle(random_indices)\n",
    "\n",
    "    # checking the run_type\n",
    "    if run_type == 'demo':\n",
    "        print(f\"Original shape of 2D samples: {x_2d.shape}\")\n",
    "        print(f\"Original shape of 3D samples: {x_3d.shape}\")\n",
    "        demo_samples = 100\n",
    "        random_indices = random_indices[0:demo_samples]\n",
    "\n",
    "    # Selecting training and test samples...\n",
    "    x_2d = x_2d[random_indices]\n",
    "    x_3d = x_3d[random_indices]\n",
    "    y = y[random_indices]\n",
    "   \n",
    "    # Done loading and processing dataset...\n",
    "    # print(f\"{data_type} data loaded...!\")\n",
    "    print(f\"_______________________________________________\")\n",
    "    print(f\"Loaded shape of 2D samples: {x_2d.shape}\")\n",
    "    print(f\"Loaded shape of 3D samples: {x_3d.shape}\")\n",
    "    print(\"\\n\")\n",
    "    # return image, pointclouds, and labels\n",
    "    return x_2d, x_3d, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading training data\n",
    "x_train_2d, x_train_3d, y_train = load_data(data_type = 'train', num_pcs = num_cloud_points)\n",
    "y_train_digit = np.argmax(y_train, axis=1) # Convert one-hot to index\n",
    "\n",
    "# Loading test data\n",
    "x_test_2d, x_test_3d, y_test = load_data(data_type = 'test', num_pcs = num_cloud_points)\n",
    "y_test_digit = np.argmax(y_test, axis=1) # Convert one-hot to index\n",
    "y_test_df = pd.DataFrame(y_test_digit, columns = ['Label'])\n",
    "\n",
    "x_train_dict = {}\n",
    "x_train_dict['2D'] = x_train_2d\n",
    "x_train_dict['3D'] = x_train_3d\n",
    "x_train_dict['Fusion'] = [x_train_2d, x_train_3d]\n",
    "\n",
    "x_test_dict = {}\n",
    "x_test_dict['2D'] = x_test_2d\n",
    "x_test_dict['3D'] = x_test_3d\n",
    "x_test_dict['Fusion'] = [x_test_2d, x_test_3d]\n",
    "\n",
    "# Creating tensor for adv sample generation\n",
    "y_test_df_samp = y_test_df.groupby('Label', group_keys=False).apply(lambda x: x.sample(1))\n",
    "test_ind = y_test_df_samp.index\n",
    "\n",
    "# Creating test samples with statified sampling...\n",
    "# x_test_2d_tesnor = tf.convert_to_tensor(x_test_2d[test_ind])\n",
    "# x_test_3d_tesnor = tf.convert_to_tensor(x_test_3d[test_ind])\n",
    "# x_test_fusion_tesnor = [x_test_2d_tesnor, x_test_3d_tesnor]\n",
    "# input_label = tf.convert_to_tensor(y_test[test_ind])\n",
    "x_actual_dict = {}\n",
    "x_actual_dict['2D'] = tf.convert_to_tensor(x_test_2d[test_ind])\n",
    "x_actual_dict['3D'] = tf.convert_to_tensor(x_test_3d[test_ind])\n",
    "x_actual_dict['Fusion'] = [x_actual_dict['2D'], x_actual_dict['3D']]\n",
    "y_actual = tf.convert_to_tensor(y_test[test_ind])\n",
    "y_actual_digit = y_test_df_samp['Label'].values\n",
    "\n",
    "\n",
    "# Loss object..\n",
    "loss_func = tf.keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Models to extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for building the models.......\n",
    "\n",
    "# Convolution with batch normalization...\n",
    "def conv_bn(x, filters):\n",
    "    x = layers.Conv1D(filters, kernel_size=1, padding=\"valid\")(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
    "    return layers.Activation(\"relu\")(x)\n",
    "\n",
    "# Dense with batch normalization...\n",
    "def dense_bn(x, filters):\n",
    "    x = layers.Dense(filters)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
    "    return layers.Activation(\"relu\")(x)\n",
    "\n",
    "# Transormation Network (T-Net)...\n",
    "def tnet(inputs, num_features):\n",
    "    # Initalise bias as the indentity matrix\n",
    "    bias = keras.initializers.Constant(np.eye(num_features).flatten())\n",
    "    # reg = OrthogonalRegularizer(num_features)\n",
    "    x = conv_bn(inputs, 32)\n",
    "    x = conv_bn(x, 64)\n",
    "    x = conv_bn(x, 512)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = dense_bn(x, 256)\n",
    "    x = dense_bn(x, 128)\n",
    "    x = layers.Dense(\n",
    "        num_features * num_features,\n",
    "        kernel_initializer=\"zeros\",\n",
    "        bias_initializer=bias,\n",
    "        # activity_regularizer=reg,\n",
    "    )(x)\n",
    "    feat_T = layers.Reshape((num_features, num_features))(x)\n",
    "    # Apply affine transformation to input features\n",
    "    return layers.Dot(axes=(2, 1))([inputs, feat_T])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model to extract 128 features from 2D MNIST dataset\n",
    "inputs_2D = Input(shape=image_size)\n",
    "feat_2d = Conv2D(32, kernel_size=(3, 3), activation=\"relu\")(inputs_2D)\n",
    "feat_2d = MaxPooling2D(pool_size=(2, 2))(feat_2d)\n",
    "feat_2d = Conv2D(64, kernel_size=(3, 3), activation=\"relu\")(feat_2d)\n",
    "feat_2d = MaxPooling2D(pool_size=(2, 2))(feat_2d)\n",
    "feat_2d = Flatten()(feat_2d)\n",
    "feat_2d = Dropout(0.3)(feat_2d)\n",
    "feat_2d = Dense(128, activation=\"relu\")(feat_2d)\n",
    "# Creating the model to extract features from input images\n",
    "model_feat_ext_2D = Model(inputs=inputs_2D, outputs=feat_2d,  name=\"feature-extractor-2D\")\n",
    "print(model_feat_ext_2D.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pointnet model to extract 128 features from 3D MNIST dataset\n",
    "inputs_3D = Input(shape= point_cloud_size)\n",
    "\n",
    "feat_3D = tnet(inputs_3D, 3)\n",
    "feat_3D = conv_bn(feat_3D, 32)\n",
    "feat_3D = conv_bn(feat_3D, 32)\n",
    "feat_3D = tnet(feat_3D, 32)\n",
    "feat_3D = conv_bn(feat_3D, 32)\n",
    "feat_3D = conv_bn(feat_3D, 64)\n",
    "feat_3D = conv_bn(feat_3D, 512)\n",
    "feat_3D = GlobalMaxPooling1D()(feat_3D)\n",
    "feat_3D = dense_bn(feat_3D, 256)\n",
    "feat_3D = Dropout(0.3)(feat_3D)\n",
    "feat_3D = dense_bn(feat_3D, 128)\n",
    "# Creating the model to extract features from input point cluds\n",
    "model_feat_ext_3D = Model(inputs=inputs_3D, outputs=feat_3D,  name=\"feature-extractor-3D\")\n",
    "print(model_feat_ext_3D.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model_name, model_type, model, epochs, batch_size, patience):\n",
    "    \"\"\"  \n",
    "    arg: model and other info\n",
    "    return: trained model & eval data\n",
    "    \"\"\"\n",
    "    # Train and evaluate model\n",
    "    # Starting the training...\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=patience)\n",
    "    history = model.fit(x_train_dict[model_type], y_train, batch_size=batch_size\n",
    "    , epochs=epochs, validation_split=0.1, callbacks=[callback])\n",
    "\n",
    "    # Evaluate model on test data...\n",
    "    accuracy = model.evaluate(x_test_dict[model_type], y_test, batch_size=56)[1]\n",
    "    print(f\"Test Accuracy of {model_name}: {np.round(accuracy*100,5)}%\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'], marker = 'p')\n",
    "    plt.title(f\"Test Accuracy of {model_name}: {np.round(accuracy*100,5)}%\")\n",
    "    plt.show()\n",
    "\n",
    "    # Classification matrix...\n",
    "    y_pred = np.argmax(model.predict(x_test_dict[model_type]), axis=1)\n",
    "    cls_rep = classification_report(y_test_digit, y_pred) \n",
    "\n",
    "    model_eval = {\n",
    "        'model': model,\n",
    "        'model_name' : model_name,\n",
    "        'model_type': model_type,\n",
    "        'history': history,\n",
    "        'accuracy': accuracy,\n",
    "        'cls_rep' : cls_rep\n",
    "    }   \n",
    "    #-------------------------\n",
    "    return model_eval\n",
    "\n",
    "def visualize_prediction(model_name, model_type, model):\n",
    "    \"\"\" \n",
    "    Visualize the prediction performance\n",
    "    arg: \n",
    "    model_type: '2D' , '3D' or 'Fusion'\n",
    "    model: keras model\n",
    "\n",
    "    return:\n",
    "    plot the first 10 samples with predicted labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Visualize predictions and images...\n",
    "    noOfsamples = 5 if model_type =='Fusion' else 10\n",
    "    images, points, labels = x_test_2d[:noOfsamples], x_test_3d[:noOfsamples], y_test[:noOfsamples]\n",
    "\n",
    "    # Setting up input data for the prediction\n",
    "    if model_type == '2D':\n",
    "        input_data = images\n",
    "    elif model_type == '3D':\n",
    "        input_data = points\n",
    "    elif model_type =='Fusion':\n",
    "        input_data = [images, points]\n",
    "        \n",
    "\n",
    "    # run test data through model\n",
    "    preds = model.predict(input_data)\n",
    "    preds = tf.math.argmax(preds, -1)\n",
    "    labels = np.argmax(labels, axis = 1)\n",
    "\n",
    "    # plot points with predicted class and label\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    # Looping over the input sample to visualization\n",
    "    for i in range(noOfsamples):\n",
    "        print(f\"pred: {preds[i].numpy()}, label: {labels[i]}\") \n",
    "        \n",
    "        if model_type == '2D' or model_type == 'Fusion':\n",
    "            if model_type != 'Fusion':\n",
    "                index = i+1\n",
    "            else:\n",
    "                index = i+1\n",
    "            ax = fig.add_subplot(2, 5, index)\n",
    "            ax.imshow(images[i, :, :, 0])\n",
    "            ax.set_title(\n",
    "                \"Pred: {:}, Label: {:}\".format(\n",
    "                    preds[i].numpy(), labels[i]\n",
    "                )\n",
    "            )\n",
    "            ax.set_axis_off()\n",
    "\n",
    "        if model_type == '3D' or model_type == 'Fusion':\n",
    "            if model_type != 'Fusion':\n",
    "                index = i+1\n",
    "            else:\n",
    "                index = noOfsamples+i+1\n",
    "            ax = fig.add_subplot(2, 5, index, projection=\"3d\")    \n",
    "            ax.scatter(points[i, :, 0], points[i, :, 1], points[i, :, 2], marker='.')\n",
    "            # ax.set_axis_off()\n",
    "            if model_type != 'Fusion':\n",
    "                ax.set_title(\"Pred: {:}, Label: {:}\".format(\n",
    "                    preds[i].numpy(), labels[i]))\n",
    "            ax.set_xticks([])\n",
    "            ax.grid(True)\n",
    "            ax.view_init(0, 10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing model eval data...\n",
    "model_eval_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating 2D CNN-based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to predict score for 128 features for MNIST\n",
    "feats_extracted_2D = Input(shape= feat_2d.shape[1:])  \n",
    "output_pred_2D = Dense(num_classes, activation=\"softmax\")(feats_extracted_2D)\n",
    "model_pred_scores_2D = Model(feats_extracted_2D, output_pred_2D)\n",
    "# model_pred_scores_2D.summary()\n",
    "\n",
    "# Model 2D CNN-based Classifier for MNIST 2D\n",
    "# Complete CNN model for MNIST 2D Dataset\n",
    "model_2D_mnist = keras.Model(inputs_2D, model_pred_scores_2D(model_feat_ext_2D(inputs_2D)))\n",
    "model_2D_mnist.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "print(model_2D_mnist.summary())\n",
    "\n",
    "\n",
    "model_name = 'model_2D_mnist'\n",
    "model = model_2D_mnist\n",
    "model_type = '2D'\n",
    "\n",
    "# Train and evaluate...\n",
    "model_eval_dict[model_name]= train_and_evaluate(\n",
    "    model_name, model_type, model, epochs, batch_size, patience)\n",
    "# # Visualize the prediction on 3D point clouds\n",
    "visualize_prediction(model_name, model_type, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating 3D Pointnet-based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to predict score for 128 features for MNIST 3D dataset\n",
    "feats_extracted_3D = Input(shape= feat_3D.shape[1:])  \n",
    "output_pred_3D = Dense(num_classes, activation=\"softmax\")(feats_extracted_3D)\n",
    "model_pred_scores_3D = Model(feats_extracted_3D, output_pred_3D)\n",
    "# model_pred_scores_3D.summary()\n",
    "\n",
    "# Model Pointnet Classifier for MNIST 3D\n",
    "model_3D_mnist = keras.Model(inputs_3D, model_pred_scores_3D(model_feat_ext_3D(inputs_3D)))\n",
    "model_3D_mnist.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "print(model_3D_mnist.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_name = 'model_3D_mnist'\n",
    "model = model_3D_mnist\n",
    "model_type = '3D'\n",
    "# epochs = 1 \n",
    "# batch_size = 32\n",
    "# patience = 5\n",
    "# x_train = x_train_3d\n",
    "# x_test = x_test_3d\n",
    "\n",
    "# Train and evaluate...\n",
    "model_eval_dict[model_name]= train_and_evaluate(\n",
    "    model_name, model_type, model, epochs, batch_size, patience)\n",
    "\n",
    "# # Visualize the prediction on 3D point clouds\n",
    "visualize_prediction(model_name, model_type, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Fusion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Late Fusion Models with different fusion layers\n",
    "from keras.layers import Concatenate, Average,Maximum, Minimum, Add, Subtract, Multiply\n",
    "\n",
    "mid_fusion_layers_dict = {\n",
    "    'concatenate': Concatenate(),\n",
    "    'Average' : Average(),\n",
    "    # 'Maximum' : Maximum(),\n",
    "    # 'Minimum' : Minimum(),\n",
    "    # 'Add': Add(),\n",
    "    # 'Subtract': Subtract(),\n",
    "    # 'Multiply': Multiply()\n",
    "}\n",
    "\n",
    "\n",
    "late_fusion_layers_dict = {\n",
    "    'Average' : Average(),\n",
    "    'Maximum' : Maximum(),\n",
    "    # 'Minimum' : Minimum(),\n",
    "    # 'Add': Add(),\n",
    "    # 'Subtract': Subtract(),\n",
    "    # 'Multiply': Multiply()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feat_ext_2D.trainable = False\n",
    "model_feat_ext_3D.trainable = False\n",
    "\n",
    "model_pred_scores_2D.trainable = False\n",
    "model_pred_scores_3D.trainable = False\n",
    "\n",
    "model_2D_mnist.trainable = False\n",
    "model_3D_mnist.trainable = False\n",
    "\n",
    "# model_feat_ext_2D.trainable = True\n",
    "# model_feat_ext_3D.trainable = True\n",
    "\n",
    "# model_pred_scores_2D.trainable = True\n",
    "# model_pred_scores_3D.trainable = True\n",
    "\n",
    "# model_2D_mnist.trainable = True\n",
    "# model_3D_mnist.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Late-Fusion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fusion_layer_name, fusion_layer in late_fusion_layers_dict.items():\n",
    "\n",
    "    print(fusion_layer_name)\n",
    "    model_name = f\"model_late_fusion_mnist_{fusion_layer_name}\"\n",
    "    \n",
    "    # combine the output of the two branches\n",
    "    output_pred_late_fusion = fusion_layer([model_2D_mnist.output, model_3D_mnist.output])\n",
    "\n",
    "    # Finally combine two models and build a single multi modal mode...\n",
    "    model_late_fusion_mnist = Model(inputs=[inputs_2D, inputs_3D], outputs= output_pred_late_fusion,  name = model_name)\n",
    "    model_late_fusion_mnist.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    # print(model_late_fusion_mnist.summary())\n",
    "\n",
    "    model_name = model_name\n",
    "    model = model_late_fusion_mnist\n",
    "    model_type = 'Fusion'\n",
    "    # epochs = 2 \n",
    "    # batch_size = 32\n",
    "    # patience = 5\n",
    "    # x_train = [x_train_2d, x_train_3d]\n",
    "    # x_test = [x_test_2d, x_test_3d]\n",
    "\n",
    "    # Train and evaluate...\n",
    "    model_eval_dict[model_name]= train_and_evaluate(\n",
    "        model_name, model_type, model, epochs, batch_size, patience)\n",
    "\n",
    "    # # Visualize the prediction on 3D point clouds\n",
    "    visualize_prediction(model_name, model_type, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Mid Fusion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fusion_layer_name, fusion_layer in mid_fusion_layers_dict.items():\n",
    "\n",
    "    print(fusion_layer_name)\n",
    "    model_name = f\"model_mid_fusion_mnist_{fusion_layer_name}\"\n",
    "    \n",
    "    # combine the output of the two branches\n",
    "    output_pred_mid_fusion = fusion_layer([model_feat_ext_2D.output, model_feat_ext_3D.output])\n",
    "    output_pred_mid_fusion = Dense(128, activation=\"relu\")(output_pred_mid_fusion)\n",
    "    output_pred_mid_fusion = Dense(64, activation=\"relu\")(output_pred_mid_fusion)\n",
    "    output_pred_mid_fusion = Dense(num_classes, activation=\"softmax\")(output_pred_mid_fusion)\n",
    "\n",
    "    # Finally combine two models and build a single multi modal mode...\n",
    "    model_mid_fusion_mnist = Model(inputs=[inputs_2D, inputs_3D], outputs= output_pred_mid_fusion,  name = model_name)\n",
    "    model_mid_fusion_mnist.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    # print(model_late_fusion_mnist.summary())\n",
    "\n",
    "    model_name = model_name\n",
    "    model = model_mid_fusion_mnist\n",
    "    model_type = 'Fusion'\n",
    "    # epochs = 2 \n",
    "    # batch_size = 32\n",
    "    # patience = 5\n",
    "    # x_train = [x_train_2d, x_train_3d]\n",
    "    # x_test = [x_test_2d, x_test_3d]\n",
    "\n",
    "    # Train and evaluate...\n",
    "    model_eval_dict[model_name]= train_and_evaluate(\n",
    "        model_name, model_type, model, epochs, batch_size, patience)\n",
    "\n",
    "    # # Visualize the prediction on 3D point clouds\n",
    "    visualize_prediction(model_name, model_type, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "for key in model_eval_dict.keys():\n",
    "    accuracy_list.append([key, model_eval_dict[key]['accuracy']])\n",
    "accuracy_df = pd.DataFrame(accuracy_list, columns = ['Name', 'Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance...\n",
    "sns.barplot(accuracy_df, y = \"Name\", x = \"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance...\n",
    "sns.scatterplot(accuracy_df, y = \"Name\", x = \"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avdersarisal attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FGSM Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data on which we will create adversarial samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_df_samp = y_test_df.groupby('Label', group_keys=False).apply(lambda x: x.sample(5))\n",
    "# test_ind = y_test_df_samp.index\n",
    "\n",
    "# # Creating test samples with statified sampling...\n",
    "# x_test_2d_tesnor = tf.convert_to_tensor(x_test_2d[test_ind])\n",
    "# x_test_3d_tesnor = tf.convert_to_tensor(x_test_3d[test_ind])\n",
    "# x_test_fusion_tesnor = [x_test_2d_tesnor, x_test_3d_tesnor]\n",
    "# input_label = tf.convert_to_tensor(y_test[test_ind])\n",
    "\n",
    "# input_tensor = x_test_fusion_tesnor\n",
    "# # Loss object..\n",
    "# loss_func = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturb_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run FGSM attacks to find purturbation matrix\n",
    "# Create adversarial samples...\n",
    "def create_adversarial_pattern(model, model_type):\n",
    "\n",
    "    input_data = x_actual_dict[model_type]\n",
    "    input_label = y_actual\n",
    "    # Dictionary to save purturb signs\n",
    "    perturbations_sign_dict = {}\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Calcuate loss...\n",
    "        tape.watch(input_data)\n",
    "        input_pred = model(input_data)\n",
    "        loss = loss_func(input_label, input_pred)\n",
    "\n",
    "        # Find gradients of the loss w.r.t to the input image.\n",
    "        gradient = tape.gradient(loss, input_data)\n",
    "\n",
    "        # Get the sign of the gradients to create the perturbation\n",
    "        if model_type == 'Fusion':\n",
    "            # split into two parts\n",
    "            perturbations_sign_dict['2D'] = tf.sign(gradient[0]).numpy()\n",
    "            perturbations_sign_dict['3D'] = tf.sign(gradient[1]).numpy()\n",
    "        else:\n",
    "            # Single output\n",
    "            perturbations_sign_dict[model_type] = tf.sign(gradient).numpy()\n",
    "\n",
    "            # Visualize purturbation...\n",
    "    return perturbations_sign_dict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilons = [0, 0.005, 0.01, 0.05, 0.1] #, 10, 100]\n",
    "epsilons = [0, 0.1] #, 10, 100]\n",
    "x_avd_sign_dict = {}\n",
    "model_eval_adv = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_eval_dict.keys():\n",
    "    \n",
    "    model = model_eval_dict[model_name]['model']\n",
    "    model_type = model_eval_dict[model_name]['model_type']\n",
    "\n",
    "    # Generate adversarial samples...\n",
    "    x_avd_sign_dict[model_name] = create_adversarial_pattern(model, model_type)\n",
    "\n",
    "    #evalaute models performance on adversarial samples..\n",
    "\n",
    "    if model_type == '2D':\n",
    "        epsilons_2d = epsilons\n",
    "        epsilons_3d = [0.0]\n",
    "    elif model_type == '3D':\n",
    "        epsilons_2d = [0.0]\n",
    "        epsilons_3d = epsilons\n",
    "    else:\n",
    "        epsilons_2d = epsilons\n",
    "        epsilons_3d = epsilons\n",
    "    \n",
    "\n",
    "    # Evaluate performance for different values of EPSILON\n",
    "    for eps_2d in tqdm(epsilons_2d):\n",
    "        for eps_3d in tqdm(epsilons_3d):\n",
    "            \n",
    "            x_del_dict = {}\n",
    "            x_adv_dict = {}\n",
    "\n",
    "            if model_type == '2D' or model_type == 'Fusion':\n",
    "                # Noise to be added...\n",
    "                x_del_dict['2D'] = x_avd_sign_dict[model_name]['2D'] * eps_2d\n",
    "                x_adv_dict['2D'] = x_actual_dict['2D'] + x_del_dict['2D']\n",
    "            \n",
    "            if model_type == '3D' or model_type == 'Fusion':\n",
    "                x_del_dict['3D'] = x_avd_sign_dict[model_name]['3D'] * eps_3d\n",
    "                x_adv_dict['3D'] = x_actual_dict['3D'] + x_del_dict['3D']\n",
    "\n",
    "            if model_type == 'Fusion':\n",
    "                x_adv_dict['Fusion'] = [x_adv_dict['2D'], x_adv_dict['3D']]\n",
    "            \n",
    "\n",
    "\n",
    "            # Evaluate model on adversarial data...\n",
    "            accuracy_adv = model.evaluate(x_adv_dict[model_type], y_actual, batch_size=32)[1]\n",
    "            print(f\"Test Accuracy of {model_name}: {np.round(accuracy_adv*100,5)}%\")\n",
    "\n",
    "            y_adv_pred = model.predict(x_adv_dict[model_type], batch_size = 32)\n",
    "\n",
    "            # Classification matrix...\n",
    "            y_adv_pred_digit = np.argmax(y_adv_pred, axis=1)\n",
    "            cls_rep_adv = classification_report(y_actual_digit, y_adv_pred_digit) \n",
    "\n",
    "\n",
    "            model_eval_adv.append([model_name, model_type, eps_2d, eps_3d, accuracy_adv, cls_rep_adv])\n",
    "\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval_adv_df = pd.DataFrame(model_eval_adv, columns = ['model_name', 'model_type', 'eps_2d', 'eps_3d', 'accuracy_adv', 'cls_rep_adv'])\n",
    "model_eval_adv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval_adv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76b18d0804b1d18cc4a8723b732596a057f641d6427a46ef33e32599b5d9a6a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
